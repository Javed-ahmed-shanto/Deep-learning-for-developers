{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Javed-ahmed-shanto/Deep-learning-for-developers/blob/main/ties4911_task05_shanto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD8VS6IkJsvC"
      },
      "source": [
        "## **Task 5-1**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ALL-IN-ONE COMPARISON SCRIPT FOR GOOGLE COLAB\n",
        "# # ----------------------------------------------\n",
        "# # This single cell demonstrates how to:\n",
        "# # 1) Install necessary libraries (Ultralytics YOLO, TorchVision)\n",
        "# # 2) Download real images from the internet\n",
        "# # 3) Load multiple object detection/segmentation models:\n",
        "# #    - YOLOv8 (Ultralytics)\n",
        "# #    - Faster R-CNN (TorchVision)\n",
        "# #    - SSD (TorchVision)\n",
        "# #    - RetinaNet (TorchVision)\n",
        "# #    - Mask R-CNN (TorchVision)\n",
        "# # 4) Run inference on ~5 real images\n",
        "# # 5) Report performance (inference time) + a simple detection statistic\n",
        "# # 6) Display bounding boxes from each model on one sample image\n",
        "# #\n",
        "# # NOTE:\n",
        "# # - This script uses pretrained COCO weights for the TorchVision models.\n",
        "# # - YOLOv8 model is also pretrained (default).\n",
        "# # - We do not compute true accuracy metrics (mAP, IoU) because we do not have GT annotations.\n",
        "# # - For more advanced usage, you would typically do:\n",
        "# #   - Evaluate on a labeled dataset.\n",
        "# #   - Possibly use GPU for faster inference.\n",
        "# #\n",
        "# # REFERENCES / IMPLEMENTATIONS:\n",
        "# # - TorchVision Detection Models:\n",
        "# #   https://pytorch.org/vision/stable/models/detection.html\n",
        "# # - Ultralytics YOLO (v8+):\n",
        "# #   https://github.com/ultralytics/ultralytics\n",
        "# # - For additional models (U-Net, YOLACT, DETR, Transformers, etc.), see their respective repos:\n",
        "# #   - U-Net: https://github.com/qubvel/segmentation_models.pytorch\n",
        "# #   - YOLACT: https://github.com/dbolya/yolact\n",
        "# #   - DETR: https://github.com/facebookresearch/detr\n",
        "\n",
        "# ################################################################################\n",
        "# # 1) INSTALL DEPENDENCIES\n",
        "# ################################################################################\n",
        "# !pip install -q ultralytics torch torchvision  # Ensure torch/torchvision, YOLO installed\n",
        "\n",
        "# ################################################################################\n",
        "# # 2) DOWNLOAD REAL IMAGES FROM THE INTERNET\n",
        "# #    We'll use a few example images from the YOLOv5 repo + other open sources.\n",
        "# ################################################################################\n",
        "# !pip install ultralytics torch torchvision\n",
        "\n",
        "# import os\n",
        "# import requests\n",
        "# from PIL import Image\n",
        "# import time\n",
        "# import torch\n",
        "# import torchvision\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # If you want to confirm file removal:\n",
        "# !rm -f test_image_0.jpg test_image_1.jpg test_image_2.jpg test_image_3.jpg test_image_4.jpg\n",
        "\n",
        "\n",
        "\n",
        "# image_urls = [\n",
        "#     \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/bus.jpg\",\n",
        "#     \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/zidane.jpg\",\n",
        "#     # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/dog.jpg\",\n",
        "#     # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/cat.jpg\",\n",
        "#     # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/person.jpg\",\n",
        "#     # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/traffic.jpg\",\n",
        "#     # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/giraffe.jpg\",\n",
        "#     # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/horse.jpg\",\n",
        "#     # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/bird.jpg\",\n",
        "#     # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/bicycle.jpg\"\n",
        "# ]\n",
        "\n",
        "\n",
        "# image_files = []\n",
        "\n",
        "# # Force re-download each file\n",
        "# for idx, url in enumerate(image_urls):\n",
        "#     out_name = f\"test_image_{idx}.jpg\"\n",
        "#     print(f\"Downloading: {url} -> {out_name}\")\n",
        "#     !wget {url} -O {out_name}  # remove -q to see output\n",
        "#     image_files.append(out_name)\n",
        "\n",
        "# print(\"\\nCheck files:\")\n",
        "# !ls -lh test_image_*.jpg\n",
        "\n",
        "# # Quick check: open each image in Python\n",
        "# for f in image_files:\n",
        "#     try:\n",
        "#         im = Image.open(f)\n",
        "#         im.verify()  # attempt to identify the file\n",
        "#         print(f\"{f} OK\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"{f} is corrupted or not an image. Error: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# # import os\n",
        "\n",
        "# # image_urls = [\n",
        "# #     \"https://ultralytics.com/images/bus.jpg\",\n",
        "# #     \"https://ultralytics.com/images/zidane.jpg\",\n",
        "# #     \"https://ultralytics.com/images/bicycle.jpg\",\n",
        "# #     \"https://ultralytics.com/images/traffic.jpg\",\n",
        "# #     \"https://ultralytics.com/images/bird.jpg\"\n",
        "# # ]\n",
        "# # image_files = []\n",
        "\n",
        "# # for idx, url in enumerate(image_urls):\n",
        "# #     out_name = f\"test_image_{idx}.jpg\"\n",
        "# #     # Force re-download every time\n",
        "# #     !wget -q {url} -O {out_name}\n",
        "# #     image_files.append(out_name)\n",
        "\n",
        "\n",
        "# ################################################################################\n",
        "# # 3) IMPORT LIBRARIES & LOAD MODELS\n",
        "# ################################################################################\n",
        "# import time\n",
        "# import torch\n",
        "# import torchvision\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "# # YOLOv8 from ultralytics\n",
        "# from ultralytics import YOLO\n",
        "\n",
        "# # TorchVision detection models\n",
        "# # NOTE: \"DEFAULT\" weights correspond to COCO-pretrained.\n",
        "# fasterrcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\").eval()\n",
        "# ssd = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=\"DEFAULT\").eval()\n",
        "# retinanet = torchvision.models.detection.retinanet_resnet50_fpn(weights=\"DEFAULT\").eval()\n",
        "# maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\").eval()\n",
        "\n",
        "# # YOLOv8 (smallest or 'n' version for speed; you can pick others, e.g. 'yolov8s.pt')\n",
        "# yolo_model = YOLO('yolov8n.pt')  # automatically downloads if not present\n",
        "\n",
        "# # Put models into a dictionary for iteration\n",
        "# models_to_evaluate = {\n",
        "#     \"YOLOv8\": yolo_model,          # via ultralytics\n",
        "#     \"FasterRCNN\": fasterrcnn,      # via TorchVision\n",
        "#     \"SSD\": ssd,                    # via TorchVision\n",
        "#     \"RetinaNet\": retinanet,        # via TorchVision\n",
        "#     \"MaskRCNN\": maskrcnn           # via TorchVision\n",
        "# }\n",
        "\n",
        "# ################################################################################\n",
        "# # 4) DEFINE INFERENCE FUNCTIONS FOR YOLO & TORCHVISION\n",
        "# ################################################################################\n",
        "\n",
        "# # For TorchVision detection models\n",
        "# def run_torchvision_inference(model, images):\n",
        "#     \"\"\"Runs inference on a list of PIL images, returns predictions and total time.\"\"\"\n",
        "#     transform = torchvision.transforms.ToTensor()\n",
        "#     input_tensors = [transform(img) for img in images]\n",
        "\n",
        "#     start_time = time.time()\n",
        "#     with torch.no_grad():\n",
        "#         preds = model(input_tensors)\n",
        "#     total_time = time.time() - start_time\n",
        "\n",
        "#     return preds, total_time\n",
        "\n",
        "# # For YOLOv8 model (ultralytics)\n",
        "# def run_yolov8_inference(model, images):\n",
        "#     \"\"\"\n",
        "#     Runs inference using ultralytics YOLO model.\n",
        "#     Returns predictions and total time.\n",
        "#     \"\"\"\n",
        "#     start_time = time.time()\n",
        "#     # YOLO can handle file paths or PIL/numpy images, but let's pass them as PIL or np arrays.\n",
        "#     # We'll collect results for all images in a single list call\n",
        "#     preds = model.predict(images, verbose=False)  # pass a list of images\n",
        "#     total_time = time.time() - start_time\n",
        "#     return preds, total_time\n",
        "\n",
        "# ################################################################################\n",
        "# # 5) RUN INFERENCE ON ALL MODELS\n",
        "# #    We'll store for each model:\n",
        "# #       - total inference time (for 5 images)\n",
        "# #       - # of detections in the first image\n",
        "# ################################################################################\n",
        "# results_table = []\n",
        "\n",
        "# # Load the 5 images into memory as PIL\n",
        "# test_images = [Image.open(f).convert(\"RGB\") for f in image_files]\n",
        "\n",
        "# for model_name, model_obj in models_to_evaluate.items():\n",
        "#     if model_name == \"YOLOv8\":\n",
        "#         # YOLO Inference\n",
        "#         preds, inf_time = run_yolov8_inference(model_obj, test_images)\n",
        "#         # preds is a list of ultralytics.yolo.engine.results.Results objects\n",
        "#         # Let's get the number of boxes from the first image:\n",
        "#         # \"boxes\" is a tensor of shape Nx6 [x1, y1, x2, y2, score, class]\n",
        "#         if len(preds) > 0:\n",
        "#             first_img_boxes = preds[0].boxes\n",
        "#             detection_count = len(first_img_boxes) if first_img_boxes is not None else 0\n",
        "#         else:\n",
        "#             detection_count = 0\n",
        "#     else:\n",
        "#         # TorchVision Inference\n",
        "#         preds, inf_time = run_torchvision_inference(model_obj, test_images)\n",
        "#         # Typically preds[i][\"boxes\"] is Nx4, so detection_count is N\n",
        "#         if len(preds) > 0 and \"boxes\" in preds[0]:\n",
        "#             detection_count = len(preds[0][\"boxes\"])\n",
        "#         else:\n",
        "#             detection_count = 0\n",
        "\n",
        "#     results_table.append({\n",
        "#         \"Model\": model_name,\n",
        "#         \"Inference Time (sec, 5 imgs)\": f\"{inf_time:.3f}\",\n",
        "#         \"Objects Detected (1st Img)\": detection_count\n",
        "#     })\n",
        "\n",
        "# df_results = pd.DataFrame(results_table)\n",
        "# print(\"=== INFERENCE RESULTS SUMMARY ===\")\n",
        "# print(df_results)\n",
        "\n",
        "# ################################################################################\n",
        "# # 6) VISUALIZE BOUNDING BOXES ON A SAMPLE IMAGE\n",
        "# #    We'll display separate plots, one for each model, for the first test image.\n",
        "# ################################################################################\n",
        "# sample_image = test_images[0]  # let's pick the first image in the list\n",
        "\n",
        "# def draw_boxes_torchvision(img, pred, score_thresh=0.5):\n",
        "#     \"\"\"\n",
        "#     For TorchVision outputs:\n",
        "#     - pred is a dict containing \"boxes\", \"scores\", ...\n",
        "#     - We'll draw boxes for scores >= threshold.\n",
        "#     \"\"\"\n",
        "#     boxes = pred[\"boxes\"]\n",
        "#     scores = pred[\"scores\"]\n",
        "\n",
        "#     np_img = np.array(img)\n",
        "#     fig, ax = plt.subplots()\n",
        "#     ax.imshow(np_img)\n",
        "\n",
        "#     for box, sc in zip(boxes, scores):\n",
        "#         if sc >= score_thresh:\n",
        "#             x1, y1, x2, y2 = box\n",
        "#             w, h = x2 - x1, y2 - y1\n",
        "#             rect = plt.Rectangle((x1, y1), w, h, fill=False, linewidth=2)\n",
        "#             ax.add_patch(rect)\n",
        "#     ax.set_title(\"TorchVision Predictions\")\n",
        "#     plt.show()\n",
        "\n",
        "# def draw_boxes_yolov8(img, yolo_result, score_thresh=0.5):\n",
        "#     import matplotlib.patches as patches\n",
        "#     import numpy as np\n",
        "#     import matplotlib.pyplot as plt\n",
        "\n",
        "#     np_img = np.array(img)\n",
        "#     fig, ax = plt.subplots()\n",
        "#     ax.imshow(np_img)\n",
        "\n",
        "#     if yolo_result.boxes is not None:\n",
        "#         for row in yolo_result.boxes.data:\n",
        "#             # Move row to CPU and convert to NumPy\n",
        "#             row = row.cpu().numpy()\n",
        "#             x1, y1, x2, y2, s, cls = row\n",
        "\n",
        "#             if s >= score_thresh:\n",
        "#                 w, h = x2 - x1, y2 - y1\n",
        "#                 rect = patches.Rectangle(\n",
        "#                     (x1, y1), w, h,\n",
        "#                     fill=False, linewidth=2\n",
        "#                 )\n",
        "#                 ax.add_patch(rect)\n",
        "\n",
        "#     ax.set_title(\"YOLOv8 Predictions\")\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# print(\"\\n=== SAMPLE VISUALIZATIONS ON FIRST IMAGE ===\")\n",
        "# for model_name, model_obj in models_to_evaluate.items():\n",
        "#     print(f\"Showing predictions for: {model_name}\")\n",
        "#     if model_name == \"YOLOv8\":\n",
        "#         preds, _ = run_yolov8_inference(model_obj, [sample_image])\n",
        "#         draw_boxes_yolov8(sample_image, preds[0], score_thresh=0.5)\n",
        "#     else:\n",
        "#         preds, _ = run_torchvision_inference(model_obj, [sample_image])\n",
        "#         draw_boxes_torchvision(sample_image, preds[0], score_thresh=0.5)\n"
      ],
      "metadata": {
        "id": "RerYsKsKMHw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "\n",
        "def install_package(package, version=None):\n",
        "    \"\"\"Install a package if not already installed, with optional version.\"\"\"\n",
        "    try:\n",
        "        pkg = f\"{package}=={version}\" if version else package\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "        print(f\"Successfully installed {pkg}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error installing {package}: {e}\")\n",
        "\n",
        "# Core dependencies\n",
        "install_package(\"ultralytics\")  # For YOLOv8\n",
        "install_package(\"torch\")\n",
        "install_package(\"torchvision\")\n",
        "install_package(\"matplotlib\")\n",
        "install_package(\"pandas\")\n",
        "install_package(\"Pillow\")\n",
        "install_package(\"requests\")\n",
        "install_package(\"numpy\")\n",
        "install_package(\"timm\")  # For EfficientDet\n",
        "\n",
        "# Additional dependencies for other models\n",
        "install_package(\"segmentation-models-pytorch\")  # For U-Net\n",
        "install_package(\"timm\")  # For advanced models\n",
        "install_package(\"transformers\")  # For DETR\n",
        "\n",
        "# Install PyTorch3D for YOLACT\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        install_package(\"pytorch3d\")\n",
        "except:\n",
        "    print(\"Skipping PyTorch3D installation - requires CUDA\")\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "# Create a directory for test images\n",
        "os.makedirs(\"test_images\", exist_ok=True)\n",
        "\n",
        "# Define a more diverse set of images (10 images as requested)\n",
        "image_urls = [\n",
        "    \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/bus.jpg\",\n",
        "    \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/zidane.jpg\",\n",
        "    # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/dog.jpg\",\n",
        "    # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/cat.jpg\",\n",
        "    # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/traffic.jpg\",\n",
        "    # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/horses.jpg\",\n",
        "    # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/motorcycle.jpg\",\n",
        "    # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/person.jpg\",\n",
        "    # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/bird.jpg\",\n",
        "    # \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/bicycle.jpg\"\n",
        "]\n",
        "\n",
        "image_files = []\n",
        "\n",
        "# Download images\n",
        "for idx, url in enumerate(image_urls):\n",
        "    out_name = f\"test_images/image_{idx}.jpg\"\n",
        "    print(f\"Downloading: {url} -> {out_name}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            with open(out_name, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "\n",
        "            # Verify the image\n",
        "            try:\n",
        "                img = Image.open(out_name)\n",
        "                img.verify()\n",
        "                image_files.append(out_name)\n",
        "                print(f\"{out_name} successfully downloaded and verified.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {out_name}: {e}\")\n",
        "        else:\n",
        "            print(f\"Failed to download {url}: {response.status_code}\")\n",
        "\n",
        "            # Fallback to wget\n",
        "            os.system(f\"wget {url} -O {out_name}\")\n",
        "\n",
        "            # Verify after wget\n",
        "            try:\n",
        "                img = Image.open(out_name)\n",
        "                img.verify()\n",
        "                image_files.append(out_name)\n",
        "                print(f\"{out_name} successfully downloaded with wget and verified.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {out_name} after wget: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {url}: {e}\")\n",
        "\n",
        "print(f\"\\nSuccessfully downloaded {len(image_files)} images\")\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device for PyTorch models\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "models_to_evaluate = {}\n",
        "\n",
        "# YOLOv8 models (multiple sizes)\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    yolo_sizes = ['n', 's', 'm', 'l']  # nano, small, medium, large\n",
        "    for size in yolo_sizes:\n",
        "        model_name = f\"YOLOv8-{size}\"\n",
        "        try:\n",
        "            models_to_evaluate[model_name] = YOLO(f'yolov8{size}.pt')\n",
        "            print(f\"Loaded {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {model_name}: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error importing YOLO: {e}\")\n",
        "\n",
        "# YOLOv5 (Legacy)\n",
        "try:\n",
        "    # Legacy YOLOv5 requires different import strategy\n",
        "    # We'll use the Hub model directly\n",
        "    import torch\n",
        "    yolov5 = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "    models_to_evaluate[\"YOLOv5\"] = yolov5\n",
        "    print(\"Loaded YOLOv5\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading YOLOv5: {e}\")\n",
        "\n",
        "# TorchVision models\n",
        "try:\n",
        "    models_to_evaluate[\"FasterRCNN\"] = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\").to(device).eval()\n",
        "    models_to_evaluate[\"SSD\"] = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=\"DEFAULT\").to(device).eval()\n",
        "    models_to_evaluate[\"RetinaNet\"] = torchvision.models.detection.retinanet_resnet50_fpn(weights=\"DEFAULT\").to(device).eval()\n",
        "    models_to_evaluate[\"MaskRCNN\"] = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\").to(device).eval()\n",
        "    print(\"Loaded TorchVision models\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading TorchVision models: {e}\")\n",
        "\n",
        "# DETR (DEtection TRansformer) from Facebook Research\n",
        "try:\n",
        "    detr = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "    detr.to(device).eval()\n",
        "    models_to_evaluate[\"DETR\"] = detr\n",
        "    print(\"Loaded DETR\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading DETR: {e}\")\n",
        "\n",
        "# U-Net for Segmentation\n",
        "try:\n",
        "    import segmentation_models_pytorch as smp\n",
        "    unet_model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",\n",
        "        encoder_weights=\"imagenet\",\n",
        "        in_channels=3,\n",
        "        classes=1,\n",
        "    ).to(device).eval()\n",
        "    models_to_evaluate[\"U-Net\"] = unet_model\n",
        "    print(\"Loaded U-Net\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading U-Net: {e}\")\n",
        "\n",
        "# EfficientDet (from timm)\n",
        "try:\n",
        "    import timm\n",
        "    efficientdet = torch.hub.load('rwightman/gen-efficientdet-pytorch', 'tf_efficientdet_d0', pretrained=True)\n",
        "    efficientdet.to(device).eval()\n",
        "    models_to_evaluate[\"EfficientDet\"] = efficientdet\n",
        "    print(\"Loaded EfficientDet\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading EfficientDet: {e}\")\n",
        "\n",
        "# FCOS (Fully Convolutional One-Stage Object Detection)\n",
        "try:\n",
        "    fcos = torch.hub.load('tianzhi0549/FCOS', 'fcos_r50_fpn_1x', pretrained=True)\n",
        "    fcos.to(device).eval()\n",
        "    models_to_evaluate[\"FCOS\"] = fcos\n",
        "    print(\"Loaded FCOS\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading FCOS: {e}\")\n",
        "\n",
        "# YOLACT (You Only Look At CoefficienTs)\n",
        "try:\n",
        "    yolact = torch.hub.load('dbolya/yolact', 'yolact_resnet50_54_800000', pretrained=True)\n",
        "    yolact.to(device).eval()\n",
        "    models_to_evaluate[\"YOLACT\"] = yolact\n",
        "    print(\"Loaded YOLACT\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading YOLACT: {e}\")\n",
        "\n",
        "print(f\"\\nLoaded {len(models_to_evaluate)} models for evaluation.\")\n",
        "\n",
        "# Generic function to preprocess images\n",
        "def preprocess_image(image, target_size=None):\n",
        "    \"\"\"Preprocess PIL image for model inference.\"\"\"\n",
        "    # Convert PIL Image to RGB if needed\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    # Resize if target_size is provided\n",
        "    if target_size:\n",
        "        image = image.resize(target_size)\n",
        "\n",
        "    return image\n",
        "\n",
        "# For TorchVision detection models\n",
        "def run_torchvision_inference(model, images):\n",
        "    \"\"\"Runs inference with TorchVision models.\"\"\"\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    input_tensors = [transform(img).to(device) for img in images]\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input_tensors)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return predictions, total_time\n",
        "\n",
        "# For YOLOv8 models\n",
        "def run_yolov8_inference(model, images):\n",
        "    \"\"\"Runs inference with YOLOv8 models.\"\"\"\n",
        "    start_time = time.time()\n",
        "    predictions = model.predict(images, verbose=False)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return predictions, total_time\n",
        "\n",
        "# For YOLOv5 models\n",
        "def run_yolov5_inference(model, images):\n",
        "    \"\"\"Runs inference with YOLOv5 models.\"\"\"\n",
        "    # Convert PIL images to paths or keep as PIL\n",
        "    start_time = time.time()\n",
        "    predictions = model(images)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return predictions, total_time\n",
        "\n",
        "# For DETR model\n",
        "def run_detr_inference(model, images):\n",
        "    \"\"\"Runs inference with DETR model.\"\"\"\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    input_tensors = [transform(img).unsqueeze(0).to(device) for img in images]\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        for tensor in input_tensors:\n",
        "            output = model(tensor)\n",
        "            predictions.append({\n",
        "                'boxes': output['pred_boxes'][0].cpu(),\n",
        "                'scores': output['pred_logits'][0].softmax(-1)[..., :-1].max(-1)[0].cpu(),\n",
        "                'labels': output['pred_logits'][0].softmax(-1)[..., :-1].max(-1)[1].cpu()\n",
        "            })\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return predictions, total_time\n",
        "\n",
        "# For U-Net segmentation model\n",
        "def run_unet_inference(model, images):\n",
        "    \"\"\"Runs inference with U-Net segmentation model.\"\"\"\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        torchvision.transforms.Resize((256, 256))\n",
        "    ])\n",
        "\n",
        "    input_tensors = [transform(img).unsqueeze(0).to(device) for img in images]\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        for tensor in input_tensors:\n",
        "            output = model(tensor)\n",
        "            # For semantic segmentation, we count segments differently\n",
        "            mask = (output > 0.5).float().cpu()\n",
        "            predictions.append({'masks': mask})\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return predictions, total_time\n",
        "\n",
        "# For EfficientDet\n",
        "def run_efficientdet_inference(model, images):\n",
        "    \"\"\"Runs inference with EfficientDet model.\"\"\"\n",
        "    # EfficientDet uses its own preprocessing\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        for img in images:\n",
        "            img_tensor = torchvision.transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
        "            output = model(img_tensor)\n",
        "            predictions.append({\n",
        "                'boxes': output[0]['boxes'],\n",
        "                'scores': output[0]['scores'],\n",
        "                'labels': output[0]['labels']\n",
        "            })\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return predictions, total_time\n",
        "\n",
        "# For FCOS\n",
        "def run_fcos_inference(model, images):\n",
        "    \"\"\"Runs inference with FCOS model.\"\"\"\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    input_tensors = [transform(img).unsqueeze(0).to(device) for img in images]\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        for tensor in input_tensors:\n",
        "            output = model(tensor)\n",
        "            predictions.append({\n",
        "                'boxes': output[0]['boxes'].cpu(),\n",
        "                'scores': output[0]['scores'].cpu(),\n",
        "                'labels': output[0]['labels'].cpu()\n",
        "            })\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return predictions, total_time\n",
        "\n",
        "# For YOLACT\n",
        "def run_yolact_inference(model, images):\n",
        "    \"\"\"Runs inference with YOLACT model.\"\"\"\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        for img in images:\n",
        "            img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "            classes, scores, boxes, masks = model(img_tensor)\n",
        "            predictions.append({\n",
        "                'boxes': boxes[0].cpu() if len(boxes) > 0 else torch.tensor([]),\n",
        "                'scores': scores[0].cpu() if len(scores) > 0 else torch.tensor([]),\n",
        "                'labels': classes[0].cpu() if len(classes) > 0 else torch.tensor([]),\n",
        "                'masks': masks[0].cpu() if len(masks) > 0 else torch.tensor([])\n",
        "            })\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return predictions, total_time\n",
        "\n",
        "\n",
        "# Load all images as PIL objects\n",
        "test_images = [Image.open(f).convert(\"RGB\") for f in image_files]\n",
        "\n",
        "results_table = []\n",
        "all_predictions = {}  # Store predictions for visualization\n",
        "\n",
        "for model_name, model in models_to_evaluate.items():\n",
        "    print(f\"Running inference with {model_name}...\")\n",
        "\n",
        "    try:\n",
        "        # Different handling based on model type\n",
        "        if \"YOLOv8\" in model_name:\n",
        "            predictions, inf_time = run_yolov8_inference(model, test_images)\n",
        "            detection_count = len(predictions[0].boxes) if len(predictions) > 0 and predictions[0].boxes is not None else 0\n",
        "        elif model_name == \"YOLOv5\":\n",
        "            predictions, inf_time = run_yolov5_inference(model, test_images)\n",
        "            detection_count = len(predictions.xyxy[0]) if len(predictions.xyxy) > 0 else 0\n",
        "        elif model_name in [\"FasterRCNN\", \"SSD\", \"RetinaNet\", \"MaskRCNN\"]:\n",
        "            predictions, inf_time = run_torchvision_inference(model, test_images)\n",
        "            detection_count = len(predictions[0][\"boxes\"]) if len(predictions) > 0 and \"boxes\" in predictions[0] else 0\n",
        "        elif model_name == \"DETR\":\n",
        "            predictions, inf_time = run_detr_inference(model, test_images)\n",
        "            detection_count = len(predictions[0][\"boxes\"]) if len(predictions) > 0 and \"boxes\" in predictions[0] else 0\n",
        "        elif model_name == \"U-Net\":\n",
        "            predictions, inf_time = run_unet_inference(model, test_images)\n",
        "            # For U-Net, we count segments differently\n",
        "            detection_count = torch.sum(predictions[0]['masks'] > 0.5).item() if len(predictions) > 0 else 0\n",
        "        elif model_name == \"EfficientDet\":\n",
        "            predictions, inf_time = run_efficientdet_inference(model, test_images)\n",
        "            detection_count = len(predictions[0][\"boxes\"]) if len(predictions) > 0 and \"boxes\" in predictions[0] else 0\n",
        "        elif model_name == \"FCOS\":\n",
        "            predictions, inf_time = run_fcos_inference(model, test_images)\n",
        "            detection_count = len(predictions[0][\"boxes\"]) if len(predictions) > 0 and \"boxes\" in predictions[0] else 0\n",
        "        elif model_name == \"YOLACT\":\n",
        "            predictions, inf_time = run_yolact_inference(model, test_images)\n",
        "            detection_count = len(predictions[0][\"boxes\"]) if len(predictions) > 0 and \"boxes\" in predictions[0] else 0\n",
        "        else:\n",
        "            # Generic fallback\n",
        "            print(f\"Warning: No specific handling for {model_name}, skipping\")\n",
        "            continue\n",
        "\n",
        "        # Store predictions for visualization\n",
        "        all_predictions[model_name] = predictions\n",
        "\n",
        "        # Add to results table\n",
        "        fps = len(test_images) / inf_time\n",
        "        results_table.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Inference Time (sec)\": f\"{inf_time:.3f}\",\n",
        "            \"FPS\": f\"{fps:.2f}\",\n",
        "            \"Objects Detected (1st Img)\": detection_count\n",
        "        })\n",
        "\n",
        "        print(f\"  -> {model_name}: {inf_time:.3f} sec, {detection_count} objects detected\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running inference with {model_name}: {e}\")\n",
        "\n",
        "# Create DataFrame with results\n",
        "df_results = pd.DataFrame(results_table)\n",
        "\n",
        "# Sort by inference time\n",
        "df_results = df_results.sort_values(\"Inference Time (sec)\")\n",
        "\n",
        "print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
        "print(df_results)\n",
        "\n",
        "# Save results to CSV\n",
        "df_results.to_csv(\"model_comparison_results.csv\", index=False)\n",
        "print(\"Results saved to model_comparison_results.csv\")\n",
        "\n",
        "\n",
        "def visualize_detections(image, predictions, model_name, threshold=0.3):\n",
        "    \"\"\"Visualize detections for different model types.\"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ax.imshow(np.array(image))\n",
        "\n",
        "    # Different handling based on model type\n",
        "    if \"YOLOv8\" in model_name:\n",
        "        # YOLOv8 predictions\n",
        "        if predictions.boxes is not None:\n",
        "            for box in predictions.boxes.data:\n",
        "                box = box.cpu().numpy()\n",
        "                x1, y1, x2, y2, score, class_id = box\n",
        "                if score >= threshold:\n",
        "                    rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                        fill=False, edgecolor='red', linewidth=2)\n",
        "                    ax.add_patch(rect)\n",
        "                    class_name = predictions.names[int(class_id)]\n",
        "                    ax.text(x1, y1, f\"{class_name}: {score:.2f}\",\n",
        "                            bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "    elif model_name == \"YOLOv5\":\n",
        "        # YOLOv5 predictions\n",
        "        boxes = predictions.xyxy[0]\n",
        "        for box in boxes:\n",
        "            box = box.cpu().numpy()\n",
        "            x1, y1, x2, y2, score, class_id = box\n",
        "            if score >= threshold:\n",
        "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                    fill=False, edgecolor='red', linewidth=2)\n",
        "                ax.add_patch(rect)\n",
        "                class_name = predictions.names[int(class_id)]\n",
        "                ax.text(x1, y1, f\"{class_name}: {score:.2f}\",\n",
        "                        bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "    elif model_name in [\"FasterRCNN\", \"SSD\", \"RetinaNet\", \"MaskRCNN\", \"DETR\", \"EfficientDet\", \"FCOS\", \"YOLACT\"]:\n",
        "        # TorchVision-style predictions\n",
        "        if \"boxes\" in predictions and len(predictions[\"boxes\"]) > 0:\n",
        "            boxes = predictions[\"boxes\"].cpu().numpy()\n",
        "            scores = predictions[\"scores\"].cpu().numpy()\n",
        "            labels = predictions[\"labels\"].cpu().numpy() if \"labels\" in predictions else None\n",
        "\n",
        "            # If MaskRCNN, also display masks\n",
        "            if \"masks\" in predictions and model_name == \"MaskRCNN\":\n",
        "                masks = predictions[\"masks\"].cpu().numpy()\n",
        "\n",
        "                for i, (box, score) in enumerate(zip(boxes, scores)):\n",
        "                    if score >= threshold:\n",
        "                        x1, y1, x2, y2 = box\n",
        "                        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                           fill=False, edgecolor='red', linewidth=2)\n",
        "                        ax.add_patch(rect)\n",
        "\n",
        "                        # Display label with score\n",
        "                        label_text = f\"Class {labels[i]}: {score:.2f}\" if labels is not None else f\"Score: {score:.2f}\"\n",
        "                        ax.text(x1, y1, label_text, bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "                        # Add mask if available\n",
        "                        if i < len(masks):\n",
        "                            mask = masks[i][0]\n",
        "                            mask = mask > 0.5  # Threshold\n",
        "                            masked = np.ma.masked_where(~mask, mask)\n",
        "                            ax.imshow(masked, alpha=0.5, cmap='jet')\n",
        "            else:\n",
        "                # Standard bounding box visualization\n",
        "                for i, (box, score) in enumerate(zip(boxes, scores)):\n",
        "                    if score >= threshold:\n",
        "                        x1, y1, x2, y2 = box\n",
        "                        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                           fill=False, edgecolor='red', linewidth=2)\n",
        "                        ax.add_patch(rect)\n",
        "\n",
        "                        # Display label with score\n",
        "                        label_text = f\"Class {labels[i]}: {score:.2f}\" if labels is not None else f\"Score: {score:.2f}\"\n",
        "                        ax.text(x1, y1, label_text, bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "    elif model_name == \"U-Net\":\n",
        "        # For U-Net segmentation masks\n",
        "        if \"masks\" in predictions:\n",
        "            mask = predictions[\"masks\"].squeeze().cpu().numpy()\n",
        "            mask = mask > 0.5  # Apply threshold\n",
        "            masked = np.ma.masked_where(~mask, mask)\n",
        "            ax.imshow(masked, alpha=0.5, cmap='jet')\n",
        "\n",
        "    ax.set_title(f\"{model_name} Predictions\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"detections_{model_name.replace('-', '_')}.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Sample image for visualization (first image)\n",
        "sample_image = test_images[0]\n",
        "\n",
        "# Visualize predictions from each model\n",
        "print(\"\\n=== GENERATING VISUALIZATIONS ===\")\n",
        "\n",
        "for model_name, predictions in all_predictions.items():\n",
        "    try:\n",
        "        print(f\"Visualizing {model_name} predictions...\")\n",
        "\n",
        "        if \"YOLOv8\" in model_name:\n",
        "            visualize_detections(sample_image, predictions[0], model_name)\n",
        "        elif model_name == \"YOLOv5\":\n",
        "            visualize_detections(sample_image, predictions, model_name)\n",
        "        else:\n",
        "            visualize_detections(sample_image, predictions[0], model_name)\n",
        "\n",
        "        print(f\"  -> Saved visualization for {model_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error visualizing {model_name}: {e}\")\n",
        "\n",
        "\n",
        "def create_grid_visualization():\n",
        "    \"\"\"Create a grid visualization of all model detections.\"\"\"\n",
        "    # Collect all saved visualization images\n",
        "    visualization_files = [f for f in os.listdir() if f.startswith('detections_') and f.endswith('.png')]\n",
        "\n",
        "    if len(visualization_files) == 0:\n",
        "        print(\"No visualization files found.\")\n",
        "        return\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    n_images = len(visualization_files)\n",
        "    n_cols = min(3, n_images)\n",
        "    n_rows = math.ceil(n_images / n_cols)\n",
        "\n",
        "    # Create a grid figure\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "\n",
        "    # Flatten axes if needed\n",
        "    if n_rows > 1 or n_cols > 1:\n",
        "        axes = axes.flatten()\n",
        "    else:\n",
        "        axes = [axes]\n",
        "\n",
        "    # Add each visualization to the grid\n",
        "    for i, file in enumerate(visualization_files):\n",
        "        if i < len(axes):\n",
        "            # Load image\n",
        "            img = plt.imread(file)\n",
        "            axes[i].imshow(img)\n",
        "\n",
        "            # Extract model name from filename\n",
        "            model_name = file.replace('detections_', '').replace('.png', '').replace('_', '-')\n",
        "            axes[i].set_title(model_name)\n",
        "            axes[i].axis('off')\n",
        "\n",
        "    # Hide any unused axes\n",
        "    for i in range(len(visualization_files), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"all_models_comparison.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Created grid visualization of all models: all_models_comparison.png\")\n",
        "\n",
        "create_grid_visualization()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWZyaxpYfdbp",
        "outputId": "2ff2757c-d294-43b6-aa0d-ea2980f87029"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed ultralytics\n",
            "Successfully installed torch\n",
            "Successfully installed torchvision\n",
            "Successfully installed matplotlib\n",
            "Successfully installed pandas\n",
            "Successfully installed Pillow\n",
            "Successfully installed requests\n",
            "Successfully installed numpy\n",
            "Successfully installed timm\n",
            "Successfully installed segmentation-models-pytorch\n",
            "Successfully installed timm\n",
            "Successfully installed transformers\n",
            "Error installing pytorch3d: Command '['/usr/bin/python3', '-m', 'pip', 'install', 'pytorch3d']' returned non-zero exit status 1.\n",
            "Downloading: https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/bus.jpg -> test_images/image_0.jpg\n",
            "test_images/image_0.jpg successfully downloaded and verified.\n",
            "Downloading: https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/zidane.jpg -> test_images/image_1.jpg\n",
            "test_images/image_1.jpg successfully downloaded and verified.\n",
            "\n",
            "Successfully downloaded 2 images\n",
            "Using device: cuda\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 76.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded YOLOv8-n\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 21.5M/21.5M [00:00<00:00, 138MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded YOLOv8-s\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 181MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded YOLOv8-m\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt to 'yolov8l.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83.7M/83.7M [00:00<00:00, 170MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded YOLOv8-l\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2025-3-6 Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|██████████| 14.1M/14.1M [00:00<00:00, 136MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded YOLOv5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:00<00:00, 193MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/ssdlite320_mobilenet_v3_large_coco-a79551df.pth\" to /root/.cache/torch/hub/checkpoints/ssdlite320_mobilenet_v3_large_coco-a79551df.pth\n",
            "100%|██████████| 13.4M/13.4M [00:00<00:00, 56.0MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_coco-eeacb38b.pth\n",
            "100%|██████████| 130M/130M [00:00<00:00, 190MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
            "100%|██████████| 170M/170M [00:01<00:00, 117MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded TorchVision models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/detr/zipball/main\" to /root/.cache/torch/hub/main.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading DETR: No module named 'models.backbone'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 178MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded U-Net\n",
            "Error loading EfficientDet: HTTP Error 404: Not Found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/tianzhi0549/FCOS/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading FCOS: [Errno 2] No such file or directory: '/root/.cache/torch/hub/tianzhi0549_FCOS_master/hubconf.py'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/dbolya/yolact/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading YOLACT: [Errno 2] No such file or directory: '/root/.cache/torch/hub/dbolya_yolact_master/hubconf.py'\n",
            "\n",
            "Loaded 10 models for evaluation.\n",
            "Running inference with YOLOv8-n...\n",
            "  -> YOLOv8-n: 0.638 sec, 5 objects detected\n",
            "Running inference with YOLOv8-s...\n",
            "  -> YOLOv8-s: 0.291 sec, 5 objects detected\n",
            "Running inference with YOLOv8-m...\n",
            "  -> YOLOv8-m: 0.449 sec, 5 objects detected\n",
            "Running inference with YOLOv8-l...\n",
            "  -> YOLOv8-l: 0.605 sec, 6 objects detected\n",
            "Running inference with YOLOv5...\n",
            "  -> YOLOv5: 0.029 sec, 5 objects detected\n",
            "Running inference with FasterRCNN...\n",
            "  -> FasterRCNN: 0.350 sec, 31 objects detected\n",
            "Running inference with SSD...\n",
            "  -> SSD: 0.256 sec, 300 objects detected\n",
            "Running inference with RetinaNet...\n",
            "  -> RetinaNet: 0.124 sec, 273 objects detected\n",
            "Running inference with MaskRCNN...\n",
            "  -> MaskRCNN: 0.103 sec, 32 objects detected\n",
            "Running inference with U-Net...\n",
            "  -> U-Net: 0.039 sec, 91 objects detected\n",
            "\n",
            "=== PERFORMANCE COMPARISON ===\n",
            "        Model Inference Time (sec)    FPS  Objects Detected (1st Img)\n",
            "4      YOLOv5                0.029  68.32                           5\n",
            "9       U-Net                0.039  51.64                          91\n",
            "8    MaskRCNN                0.103  19.44                          32\n",
            "7   RetinaNet                0.124  16.11                         273\n",
            "6         SSD                0.256   7.81                         300\n",
            "1    YOLOv8-s                0.291   6.88                           5\n",
            "5  FasterRCNN                0.350   5.71                          31\n",
            "2    YOLOv8-m                0.449   4.46                           5\n",
            "3    YOLOv8-l                0.605   3.31                           6\n",
            "0    YOLOv8-n                0.638   3.13                           5\n",
            "Results saved to model_comparison_results.csv\n",
            "\n",
            "=== GENERATING VISUALIZATIONS ===\n",
            "Visualizing YOLOv8-n predictions...\n",
            "  -> Saved visualization for YOLOv8-n\n",
            "Visualizing YOLOv8-s predictions...\n",
            "  -> Saved visualization for YOLOv8-s\n",
            "Visualizing YOLOv8-m predictions...\n",
            "  -> Saved visualization for YOLOv8-m\n",
            "Visualizing YOLOv8-l predictions...\n",
            "  -> Saved visualization for YOLOv8-l\n",
            "Visualizing YOLOv5 predictions...\n",
            "  -> Saved visualization for YOLOv5\n",
            "Visualizing FasterRCNN predictions...\n",
            "  -> Saved visualization for FasterRCNN\n",
            "Visualizing SSD predictions...\n",
            "  -> Saved visualization for SSD\n",
            "Visualizing RetinaNet predictions...\n",
            "  -> Saved visualization for RetinaNet\n",
            "Visualizing MaskRCNN predictions...\n",
            "  -> Saved visualization for MaskRCNN\n",
            "Visualizing U-Net predictions...\n",
            "  -> Saved visualization for U-Net\n",
            "Created grid visualization of all models: all_models_comparison.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHCwdlDoJ3GE"
      },
      "source": [
        "## **Task 5-2**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import time\n",
        "# import requests\n",
        "# import zipfile\n",
        "# import random\n",
        "# from io import BytesIO\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import torchvision.models as models\n",
        "# from torchvision.utils import save_image\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Set device to GPU if available\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# # Create directories for datasets and results\n",
        "# os.makedirs(\"data\", exist_ok=True)\n",
        "# os.makedirs(\"data/content\", exist_ok=True)\n",
        "# os.makedirs(\"data/style\", exist_ok=True)\n",
        "# os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# def download_and_extract_zip(url, extract_to):\n",
        "#     \"\"\"Download and extract a zip file from URL to the specified directory.\"\"\"\n",
        "#     try:\n",
        "#         print(f\"Downloading from {url}...\")\n",
        "#         response = requests.get(url, stream=True)\n",
        "#         z = zipfile.ZipFile(BytesIO(response.content))\n",
        "#         print(\"Extracting zip file...\")\n",
        "#         z.extractall(extract_to)\n",
        "#         print(f\"Files extracted to {extract_to}\")\n",
        "#         return True\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error downloading/extracting: {e}\")\n",
        "#         return False\n",
        "\n",
        "# # Download sample content images (COCO subset)\n",
        "# content_url = \"https://github.com/pytorch/examples/raw/main/fast_neural_style/images.zip\"\n",
        "# download_and_extract_zip(content_url, \"data/content\")\n",
        "\n",
        "# # Instead of downloading the full WikiArt dataset (which is large),\n",
        "# # we'll download a few famous paintings for style images\n",
        "# style_images = [\n",
        "#     (\"starry_night.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"),\n",
        "#     (\"the_scream.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg/800px-Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg\"),\n",
        "#     (\"picasso.jpg\", \"https://upload.wikimedia.org/wikipedia/en/thumb/9/9c/Pablo_Picasso%2C_1910%2C_Girl_with_a_Mandolin_%28Fanny_Tellier%29%2C_oil_on_canvas%2C_100.3_x_73.6_cm%2C_Museum_of_Modern_Art_New_York..jpg/800px-Pablo_Picasso%2C_1910%2C_Girl_with_a_Mandolin_%28Fanny_Tellier%29%2C_oil_on_canvas%2C_100.3_x_73.6_cm%2C_Museum_of_Modern_Art_New_York..jpg\"),\n",
        "#     (\"mona_lisa.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/800px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\"),\n",
        "#     (\"kandinsky.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/a/ad/Wassily_Kandinsky_Composition_VIII.jpg\"),\n",
        "#     (\"great_wave.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg\"),\n",
        "#     (\"candy.jpg\", \"https://github.com/pytorch/examples/blob/main/fast_neural_style/images/style-images/candy.jpg\"),\n",
        "\n",
        "\n",
        "\n",
        "# ]\n",
        "\n",
        "# for filename, url in style_images:\n",
        "#     try:\n",
        "#         print(f\"Downloading {filename}...\")\n",
        "#         response = requests.get(url)\n",
        "#         img_path = os.path.join(\"data/style\", filename)\n",
        "#         with open(img_path, \"wb\") as f:\n",
        "#             f.write(response.content)\n",
        "#         print(f\"Saved to {img_path}\")\n",
        "\n",
        "#         # Verify the image\n",
        "#         try:\n",
        "#             Image.open(img_path).verify()\n",
        "#         except:\n",
        "#             print(f\"Warning: {filename} may be corrupted\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error downloading {filename}: {e}\")\n",
        "\n",
        "# # List the downloaded images\n",
        "# print(\"\\nContent images:\")\n",
        "# content_images = [f for f in os.listdir(\"data/content\") if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "# print(content_images[:5])  # Show first 5\n",
        "\n",
        "# print(\"\\nStyle images:\")\n",
        "# style_images = [f for f in os.listdir(\"data/style\") if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "# print(style_images)\n",
        "\n",
        "\n",
        "# # Define image loading and preprocessing\n",
        "# # def load_image(image_path, max_size=512):\n",
        "# #     \"\"\"Load an image, resize it and convert to tensor.\"\"\"\n",
        "# #     image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# #     # Resize while keeping aspect ratio\n",
        "# #     if max(image.size) > max_size:\n",
        "# #         ratio = max_size / max(image.size)\n",
        "# #         new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
        "# #         image = image.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "# #     # Convert to tensor\n",
        "# #     transform = transforms.Compose([\n",
        "# #         transforms.ToTensor(),\n",
        "# #         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "# #     ])\n",
        "\n",
        "# #     # Add batch dimension\n",
        "# #     image = transform(image).unsqueeze(0)\n",
        "\n",
        "# #     return image.to(device)\n",
        "\n",
        "# def load_img(path_to_img, max_dim=512):\n",
        "#     \"\"\"Load and preprocess images to make them suitable for the model\"\"\"\n",
        "#     # Handle different input types\n",
        "#     if isinstance(path_to_img, str):\n",
        "#         if path_to_img.startswith('http'):\n",
        "#             # Download image from URL using requests with a proper User-Agent\n",
        "#             headers = {\n",
        "#                 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "#             }\n",
        "#             try:\n",
        "#                 response = requests.get(path_to_img, headers=headers, timeout=10)\n",
        "#                 response.raise_for_status()  # Check if the request was successful\n",
        "\n",
        "#                 # Use PIL to open the image\n",
        "#                 img = Image.open(BytesIO(response.content))\n",
        "#                 # Convert to numpy array and then to tensor\n",
        "#                 img = np.array(img)\n",
        "#                 img = torch.from_numpy(img).float()\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error downloading or processing image from {path_to_img}: {str(e)}\")\n",
        "#                 raise\n",
        "#         else:\n",
        "#             try:\n",
        "#                 img = Image.open(path_to_img)\n",
        "#                 img = np.array(img)\n",
        "#                 img = torch.from_numpy(img).float()\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error loading local image {path_to_img}: {str(e)}\")\n",
        "#                 raise\n",
        "#     else:\n",
        "#         img = path_to_img  # Assume it's already a tensor\n",
        "\n",
        "#     # Ensure the image has 3 channels (RGB)\n",
        "#     if img.shape[-1] == 4:  # If RGBA (with alpha channel)\n",
        "#         img = img[..., :3]  # Keep only RGB\n",
        "\n",
        "#     # Convert to float in [0, 1] range\n",
        "#     if img.max() > 1.0:\n",
        "#         img = img / 255.0\n",
        "\n",
        "#     # Resize the image while preserving aspect ratio\n",
        "#     height, width = img.shape[0], img.shape[1]\n",
        "#     long_dim = max(height, width)\n",
        "#     scale = max_dim / long_dim\n",
        "#     new_height = int(height * scale)\n",
        "#     new_width = int(width * scale)\n",
        "\n",
        "#     # Reshape and normalize\n",
        "#     img = img.permute(2, 0, 1)  # Convert to [C, H, W]\n",
        "#     img = F.interpolate(img.unsqueeze(0), size=(new_height, new_width), mode='bilinear', align_corners=False).squeeze(0)\n",
        "\n",
        "#     # Normalize using ImageNet stats\n",
        "#     mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "#     std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "#     img = (img - mean) / std\n",
        "\n",
        "#     # Add batch dimension\n",
        "#     img = img.unsqueeze(0)\n",
        "\n",
        "#     return img.to(device)\n",
        "\n",
        "# def tensor_to_image(tensor):\n",
        "#     \"\"\"Convert tensor to PIL Image.\"\"\"\n",
        "#     # Clone the tensor to avoid modifying the original\n",
        "#     tensor = tensor.clone().detach().cpu()\n",
        "\n",
        "#     # Denormalize\n",
        "#     tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "\n",
        "#     # Clamp to [0, 1]\n",
        "#     tensor = torch.clamp(tensor, 0, 1)\n",
        "\n",
        "#     # Convert to PIL image\n",
        "#     tensor = tensor.squeeze().permute(1, 2, 0).numpy() * 255\n",
        "#     return Image.fromarray(tensor.astype('uint8'))\n",
        "\n",
        "# # Select random content and style images\n",
        "# def get_random_images(num_content=1, num_style=1):\n",
        "#     \"\"\"Select random content and style images.\"\"\"\n",
        "#     content_paths = [os.path.join(\"data/content\", img) for img in\n",
        "#                      random.sample(content_images, min(num_content, len(content_images)))]\n",
        "\n",
        "#     style_paths = [os.path.join(\"data/style\", img) for img in\n",
        "#                    random.sample(style_images, min(num_style, len(style_images)))]\n",
        "\n",
        "#     return content_paths, style_paths\n",
        "\n",
        "# # VGG19 feature extraction\n",
        "# class VGGFeatures(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(VGGFeatures, self).__init__()\n",
        "#         # Load pretrained VGG19 and extract specific layers\n",
        "#         vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "\n",
        "#         # Freeze parameters\n",
        "#         for param in vgg.parameters():\n",
        "#             param.requires_grad = False\n",
        "\n",
        "#         # Feature layers we're interested in\n",
        "#         self.layers = {\n",
        "#             '0': 'conv1_1',\n",
        "#             '5': 'conv2_1',\n",
        "#             '10': 'conv3_1',\n",
        "#             '19': 'conv4_1',\n",
        "#             '21': 'conv4_2',  # content representation\n",
        "#             '28': 'conv5_1'\n",
        "#         }\n",
        "\n",
        "#         self.model = vgg\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         features = {}\n",
        "#         for name, layer in self.model._modules.items():\n",
        "#             x = layer(x)\n",
        "#             if name in self.layers:\n",
        "#                 features[self.layers[name]] = x\n",
        "\n",
        "#         return features\n",
        "\n",
        "# # Gram Matrix for style representation\n",
        "# def gram_matrix(tensor):\n",
        "#     \"\"\"Calculate Gram Matrix for style representation.\"\"\"\n",
        "#     batch, channel, height, width = tensor.size()\n",
        "#     tensor = tensor.view(batch, channel, height * width)\n",
        "\n",
        "#     # Compute gram matrix\n",
        "#     gram = torch.bmm(tensor, tensor.transpose(1, 2))\n",
        "\n",
        "#     # Normalize by total elements\n",
        "#     return gram.div(channel * height * width)\n",
        "\n",
        "# def classic_style_transfer(content_img, style_img, num_steps=300, style_weight=1e6, content_weight=1):\n",
        "#     \"\"\"Implement the classic Neural Style Transfer algorithm by Gatys et al.\"\"\"\n",
        "\n",
        "#     print(\"\\nRunning Classic Neural Style Transfer (Gatys et al.)...\")\n",
        "\n",
        "#     # Initialize VGG model for feature extraction\n",
        "#     vgg = VGGFeatures().to(device)\n",
        "\n",
        "#     # Create a target image initialized with the content image\n",
        "#     target = content_img.clone().requires_grad_(True).to(device)\n",
        "\n",
        "#     # Optimizer\n",
        "#     optimizer = optim.LBFGS([target])\n",
        "\n",
        "#     # Extract features\n",
        "#     content_features = vgg(content_img)\n",
        "#     style_features = vgg(style_img)\n",
        "\n",
        "#     # Calculate style gram matrices\n",
        "#     style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "#     # Style and content layer weights\n",
        "#     style_layer_weights = {'conv1_1': 1.0, 'conv2_1': 0.8, 'conv3_1': 0.6, 'conv4_1': 0.4, 'conv5_1': 0.2}\n",
        "#     content_layer = 'conv4_2'\n",
        "\n",
        "#     # Progress bar\n",
        "#     progress_bar = tqdm(range(num_steps), desc=\"Style Transfer Progress\")\n",
        "\n",
        "#     # Style transfer loop\n",
        "#     count = [0]  # For closure\n",
        "\n",
        "#     def closure():\n",
        "#         # Zero the gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Forward pass\n",
        "#         target_features = vgg(target)\n",
        "\n",
        "#         # Content loss\n",
        "#         content_loss = F.mse_loss(target_features[content_layer], content_features[content_layer])\n",
        "\n",
        "#         # Style loss\n",
        "#         style_loss = 0\n",
        "#         for layer in style_layer_weights:\n",
        "#             target_feature = target_features[layer]\n",
        "#             target_gram = gram_matrix(target_feature)\n",
        "#             style_gram = style_grams[layer]\n",
        "#             layer_style_loss = style_layer_weights[layer] * F.mse_loss(target_gram, style_gram)\n",
        "#             style_loss += layer_style_loss\n",
        "\n",
        "#         # Total loss\n",
        "#         total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "\n",
        "#         # Backward pass\n",
        "#         total_loss.backward()\n",
        "\n",
        "#         # Update progress\n",
        "#         count[0] += 1\n",
        "#         if count[0] % 50 == 0 or count[0] == num_steps:\n",
        "#             progress_bar.update(50 if count[0] != num_steps else num_steps % 50 or 50)\n",
        "#             progress_bar.set_postfix(\n",
        "#                 content_loss=f\"{content_loss.item():.4f}\",\n",
        "#                 style_loss=f\"{style_loss.item():.4f}\",\n",
        "#                 total_loss=f\"{total_loss.item():.4f}\"\n",
        "#             )\n",
        "\n",
        "#         return total_loss\n",
        "\n",
        "#     # Run optimization\n",
        "#     for _ in range(num_steps // 50):\n",
        "#         optimizer.step(closure)\n",
        "\n",
        "#     progress_bar.close()\n",
        "\n",
        "#     # Convert result to image\n",
        "#     result_img = tensor_to_image(target.detach())\n",
        "\n",
        "#     return result_img\n",
        "\n",
        "\n",
        "# # Define the Transformer Network for fast neural style transfer\n",
        "# class TransformerNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(TransformerNet, self).__init__()\n",
        "\n",
        "#         # Initial convolution layers\n",
        "#         self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
        "#         self.in1 = nn.InstanceNorm2d(32, affine=True)\n",
        "#         self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
        "#         self.in2 = nn.InstanceNorm2d(64, affine=True)\n",
        "#         self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
        "#         self.in3 = nn.InstanceNorm2d(128, affine=True)\n",
        "\n",
        "#         # Residual blocks\n",
        "#         self.res1 = ResidualBlock(128)\n",
        "#         self.res2 = ResidualBlock(128)\n",
        "#         self.res3 = ResidualBlock(128)\n",
        "#         self.res4 = ResidualBlock(128)\n",
        "#         self.res5 = ResidualBlock(128)\n",
        "\n",
        "#         # Upsampling layers\n",
        "#         self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "#         self.in4 = nn.InstanceNorm2d(64, affine=True)\n",
        "#         self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "#         self.in5 = nn.InstanceNorm2d(32, affine=True)\n",
        "#         self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
        "\n",
        "#         # Non-linearities\n",
        "#         self.relu = nn.ReLU()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         y = self.relu(self.in1(self.conv1(x)))\n",
        "#         y = self.relu(self.in2(self.conv2(y)))\n",
        "#         y = self.relu(self.in3(self.conv3(y)))\n",
        "#         y = self.res1(y)\n",
        "#         y = self.res2(y)\n",
        "#         y = self.res3(y)\n",
        "#         y = self.res4(y)\n",
        "#         y = self.res5(y)\n",
        "#         y = self.relu(self.in4(self.deconv1(y)))\n",
        "#         y = self.relu(self.in5(self.deconv2(y)))\n",
        "#         y = self.deconv3(y)\n",
        "#         return y\n",
        "\n",
        "# class ConvLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "#         super(ConvLayer, self).__init__()\n",
        "#         padding = kernel_size // 2\n",
        "#         self.reflection_pad = nn.ReflectionPad2d(padding)\n",
        "#         self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.reflection_pad(x)\n",
        "#         out = self.conv2d(out)\n",
        "#         return out\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "#     def __init__(self, channels):\n",
        "#         super(ResidualBlock, self).__init__()\n",
        "#         self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "#         self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
        "#         self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "#         self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
        "#         self.relu = nn.ReLU()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         residual = x\n",
        "#         out = self.relu(self.in1(self.conv1(x)))\n",
        "#         out = self.in2(self.conv2(out))\n",
        "#         out = out + residual\n",
        "#         return out\n",
        "\n",
        "# class UpsampleConvLayer(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "#         super(UpsampleConvLayer, self).__init__()\n",
        "#         self.upsample = upsample\n",
        "#         reflection_padding = kernel_size // 2\n",
        "#         self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "#         self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x_in = x\n",
        "#         if self.upsample:\n",
        "#             x_in = F.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "#         out = self.reflection_pad(x_in)\n",
        "#         out = self.conv2d(out)\n",
        "#         return out\n",
        "\n",
        "# def train_fast_style_transfer(style_img, dataset_path, epochs=2, batch_size=4):\n",
        "#     \"\"\"Train a Fast Neural Style Transfer model.\"\"\"\n",
        "\n",
        "#     print(\"\\nTraining Fast Neural Style Transfer model (Johnson et al.)...\")\n",
        "\n",
        "#     # Define model and optimizer\n",
        "#     transformer = TransformerNet().to(device)\n",
        "#     optimizer = optim.Adam(transformer.parameters(), 1e-3)\n",
        "\n",
        "#     # Load VGG for feature extraction\n",
        "#     vgg = VGGFeatures().to(device)\n",
        "\n",
        "#     # Extract style features\n",
        "#     style_features = vgg(style_img)\n",
        "#     style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "#     # Style and content layer weights\n",
        "#     style_layer_weights = {'conv1_1': 1.0, 'conv2_1': 0.8, 'conv3_1': 0.6, 'conv4_1': 0.4, 'conv5_1': 0.2}\n",
        "#     content_layer = 'conv4_2'\n",
        "\n",
        "#     # Define content dataset\n",
        "#     content_transform = transforms.Compose([\n",
        "#         transforms.Resize(256),\n",
        "#         transforms.CenterCrop(256),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#     ])\n",
        "\n",
        "#     # Use a folder of images as the dataset\n",
        "#     content_dataset = torchvision.datasets.ImageFolder(\n",
        "#         root=os.path.dirname(dataset_path),\n",
        "#         transform=content_transform\n",
        "#     )\n",
        "\n",
        "#     # Choose a subset for faster training\n",
        "#     subset_indices = list(range(min(100, len(content_dataset))))\n",
        "#     content_dataset = torch.utils.data.Subset(content_dataset, subset_indices)\n",
        "\n",
        "#     # Create data loader\n",
        "#     content_loader = torch.utils.data.DataLoader(\n",
        "#         content_dataset, batch_size=batch_size, shuffle=True\n",
        "#     )\n",
        "\n",
        "#     # Define loss weights\n",
        "#     style_weight = 1e5\n",
        "#     content_weight = 1\n",
        "\n",
        "#     # Training loop\n",
        "#     for epoch in range(epochs):\n",
        "#         transformer.train()\n",
        "#         total_content_loss = 0\n",
        "#         total_style_loss = 0\n",
        "#         count = 0\n",
        "\n",
        "#         # Progress bar\n",
        "#         progress_bar = tqdm(content_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "#         for batch_id, (x, _) in enumerate(progress_bar):\n",
        "#             # Move batch to device\n",
        "#             x = x.to(device)\n",
        "#             count += len(x)\n",
        "\n",
        "#             # Generate stylized images\n",
        "#             y = transformer(x)\n",
        "\n",
        "#             # Extract features\n",
        "#             y_features = vgg(y)\n",
        "#             x_features = vgg(x)\n",
        "\n",
        "#             # Content loss\n",
        "#             content_loss = F.mse_loss(y_features[content_layer], x_features[content_layer])\n",
        "\n",
        "#             # Style loss\n",
        "#             style_loss = 0\n",
        "#             for layer in style_layer_weights:\n",
        "#                 y_feature = y_features[layer]\n",
        "#                 y_gram = gram_matrix(y_feature)\n",
        "#                 style_gram = style_grams[layer].repeat(len(y), 1, 1)\n",
        "#                 layer_style_loss = style_layer_weights[layer] * F.mse_loss(y_gram, style_gram)\n",
        "#                 style_loss += layer_style_loss\n",
        "\n",
        "#             # Total loss\n",
        "#             loss = content_weight * content_loss + style_weight * style_loss\n",
        "\n",
        "#             # Update\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # Update statistics\n",
        "#             total_content_loss += content_loss.item() * len(x)\n",
        "#             total_style_loss += style_loss.item() * len(x)\n",
        "\n",
        "#             # Update progress bar\n",
        "#             progress_bar.set_postfix(\n",
        "#                 content_loss=f\"{total_content_loss/count:.4f}\",\n",
        "#                 style_loss=f\"{total_style_loss/count:.4f}\"\n",
        "#             )\n",
        "\n",
        "#     # Save the model\n",
        "#     transformer.eval()\n",
        "#     style_name = os.path.splitext(os.path.basename(style_img.name))[0]\n",
        "#     model_path = f\"fast_style_{style_name}.pth\"\n",
        "#     torch.save(transformer.state_dict(), model_path)\n",
        "#     print(f\"Model saved to {model_path}\")\n",
        "\n",
        "#     return transformer\n",
        "\n",
        "# def apply_fast_style_transfer(content_img, transformer):\n",
        "#     \"\"\"Apply a trained Fast Neural Style Transfer model to an image.\"\"\"\n",
        "\n",
        "#     print(\"\\nApplying Fast Neural Style Transfer...\")\n",
        "\n",
        "#     # Ensure model is in eval mode\n",
        "#     transformer.eval()\n",
        "\n",
        "#     # Process the image\n",
        "#     with torch.no_grad():\n",
        "#         output = transformer(content_img)\n",
        "\n",
        "#     # Convert to image\n",
        "#     result_img = tensor_to_image(output)\n",
        "\n",
        "#     return result_img\n",
        "\n",
        "# class AdaINStyleTransfer(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(AdaINStyleTransfer, self).__init__()\n",
        "#         # Load pretrained VGG19 encoder\n",
        "#         vgg = models.vgg19(pretrained=True).features\n",
        "#         self.encoder = nn.Sequential()\n",
        "\n",
        "#         # Use layers up to relu4_1\n",
        "#         for i in range(21):\n",
        "#             self.encoder.add_module(str(i), vgg[i])\n",
        "\n",
        "#         # Freeze encoder weights\n",
        "#         for param in self.encoder.parameters():\n",
        "#             param.requires_grad = False\n",
        "\n",
        "#         # Define decoder (mirror of encoder)\n",
        "#         self.decoder = nn.Sequential(\n",
        "#             nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "#             nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "#             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, content, style, alpha=1.0):\n",
        "#         # Extract features\n",
        "#         content_feat = self.encoder(content)\n",
        "#         style_feat = self.encoder(style)\n",
        "\n",
        "#         # Apply AdaIN transformation\n",
        "#         t = self.adain(content_feat, style_feat)\n",
        "\n",
        "#         # Adjust stylization strength\n",
        "#         t = alpha * t + (1 - alpha) * content_feat\n",
        "\n",
        "#         # Decode the features\n",
        "#         output = self.decoder(t)\n",
        "\n",
        "#         return output\n",
        "\n",
        "#     def adain(self, content_feat, style_feat):\n",
        "#         \"\"\"Adaptive Instance Normalization.\"\"\"\n",
        "#         size = content_feat.size()\n",
        "\n",
        "#         # Calculate mean and standard deviation of content and style features\n",
        "#         content_mean, content_std = self.calc_mean_std(content_feat)\n",
        "#         style_mean, style_std = self.calc_mean_std(style_feat)\n",
        "\n",
        "#         # Normalize content feature\n",
        "#         normalized_feat = (content_feat - content_mean.expand(size)) / content_std.expand(size)\n",
        "\n",
        "#         # Scale and shift with style statistics\n",
        "#         return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
        "\n",
        "#     def calc_mean_std(self, feat, eps=1e-5):\n",
        "#         \"\"\"Calculate mean and standard deviation of feature map.\"\"\"\n",
        "#         # Flatten [N, C, H, W] to [N, C, H*W]\n",
        "#         size = feat.size()\n",
        "#         feat_reshaped = feat.view(size[0], size[1], -1)\n",
        "\n",
        "#         # Calculate mean and standard deviation\n",
        "#         mean = feat_reshaped.mean(dim=2).view(size[0], size[1], 1, 1)\n",
        "#         std = feat_reshaped.std(dim=2).view(size[0], size[1], 1, 1) + eps\n",
        "\n",
        "#         return mean, std\n",
        "\n",
        "# def adain_style_transfer(content_img, style_img, alpha=1.0):\n",
        "#     \"\"\"Apply AdaIN style transfer.\"\"\"\n",
        "\n",
        "#     print(\"\\nRunning AdaIN Style Transfer...\")\n",
        "\n",
        "#     # Initialize model\n",
        "#     adain_model = AdaINStyleTransfer().to(device)\n",
        "\n",
        "#     # Ensure model is in eval mode\n",
        "#     adain_model.eval()\n",
        "\n",
        "#     # Process images\n",
        "#     with torch.no_grad():\n",
        "#         output = adain_model(content_img, style_img, alpha)\n",
        "\n",
        "#     # Convert to image\n",
        "#     result_img = tensor_to_image(output)\n",
        "\n",
        "#     return result_img\n",
        "\n",
        "\n",
        "# def load_pretrained_style_model(style_name):\n",
        "#     \"\"\"Load a pretrained style transfer model.\"\"\"\n",
        "\n",
        "#     # Define the model\n",
        "#     style_model = TransformerNet().to(device)\n",
        "\n",
        "#     # Try to download the model from torch hub\n",
        "#     try:\n",
        "#         # Check if the model exists locally first\n",
        "#         model_path = f\"pretrained_{style_name}.pth\"\n",
        "\n",
        "#         if os.path.exists(model_path):\n",
        "#             # Load from local file\n",
        "#             style_model.load_state_dict(torch.load(model_path))\n",
        "#             print(f\"Loaded pretrained style model from {model_path}\")\n",
        "#         else:\n",
        "#             # Try to load from torch hub\n",
        "#             model_url = f\"https://github.com/rrmina/fast-neural-style-pytorch/tree/master/transforms/{style_name}.pth\"\n",
        "\n",
        "#             print(f\"Downloading pretrained model from {model_url}\")\n",
        "\n",
        "#             response = requests.get(model_url)\n",
        "#             if response.status_code == 200:\n",
        "#                 # Save the model locally\n",
        "#                 with open(model_path, \"wb\") as f:\n",
        "#                     f.write(response.content)\n",
        "\n",
        "#                 # Load the model\n",
        "#                 style_model.load_state_dict(torch.load(model_path))\n",
        "#                 print(f\"Downloaded and loaded pretrained style model\")\n",
        "#             else:\n",
        "#                 print(f\"Failed to download pretrained model: {response.status_code}\")\n",
        "#                 return None\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading pretrained model: {e}\")\n",
        "#         return None\n",
        "\n",
        "#     return style_model\n",
        "\n",
        "# def apply_pretrained_style(content_img, style_name):\n",
        "#     \"\"\"Apply a pretrained style to the content image.\"\"\"\n",
        "\n",
        "#     print(f\"\\nApplying pretrained '{style_name}' style...\")\n",
        "\n",
        "#     # Load the pretrained model\n",
        "#     model = load_pretrained_style_model(style_name)\n",
        "\n",
        "#     if model is None:\n",
        "#         print(\"Failed to load pretrained model\")\n",
        "#         return None\n",
        "\n",
        "#     # Set to evaluation mode\n",
        "#     model.eval()\n",
        "\n",
        "#     # Process the image\n",
        "#     with torch.no_grad():\n",
        "#         output = model(content_img)\n",
        "\n",
        "#     # Convert to image\n",
        "#     result_img = tensor_to_image(output)\n",
        "\n",
        "#     return result_img\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     \"\"\"Run style transfer with multiple methods.\"\"\"\n",
        "\n",
        "#     # Since content images failed to download, let's create some manually\n",
        "#     print(\"Downloading additional images for content...\")\n",
        "#     content_urls = [\n",
        "#         (\"landscape.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/d/dd/Lake-and-landscape.jpg_-_panoramio.jpg\"),\n",
        "#         (\"portrait.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/800px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\"),\n",
        "#         (\"beach.jpg\", \"https://www.google.com/imgres?q=sea%20images&imgurl=https%3A%2F%2Fstatic.vecteezy.com%2Fsystem%2Fresources%2Fpreviews%2F015%2F275%2F318%2Fnon_2x%2Fbeautiful-beach-tropical-beach-background-as-summer-landscape-white-sand-and-calm-sea-for-beach-banner-perfect-beach-scene-vacation-and-summer-holiday-concept-boost-up-color-process-photo.jpg&imgrefurl=https%3A%2F%2Fwww.vecteezy.com%2Ffree-photos%2Fsea&docid=otMb8OOdn4XEOM&tbnid=-F5bfAOkKSaIoM&vet=12ahUKEwjy7fmbm_SLAxVmGxAIHTZfARIQM3oFCIMBEAA..i&w=1742&h=980&hcb=2&ved=2ahUKEwjy7fmbm_SLAxVmGxAIHTZfARIQM3oFCIMBEAA\")\n",
        "#     ]\n",
        "\n",
        "#     for filename, url in content_urls:\n",
        "#         try:\n",
        "#             print(f\"Downloading {filename} for content...\")\n",
        "#             response = requests.get(url)\n",
        "#             img_path = os.path.join(\"data/content\", filename)\n",
        "#             with open(img_path, \"wb\") as f:\n",
        "#                 f.write(response.content)\n",
        "#             print(f\"Saved to {img_path}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error downloading {filename}: {e}\")\n",
        "\n",
        "#     # Refresh content image list\n",
        "#     global content_images\n",
        "#     content_images = [f for f in os.listdir(\"data/content\") if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "#     print(\"\\nContent images:\")\n",
        "#     print(content_images)\n",
        "\n",
        "#     # Check if content images are available\n",
        "#     if not content_images:\n",
        "#         print(\"Warning: No content images downloaded. Using style images as content.\")\n",
        "#         # Use some style images as content\n",
        "#         content_paths = [os.path.join(\"data/style\", img) for img in style_images[:2]]\n",
        "#         style_paths = [os.path.join(\"data/style\", img) for img in style_images[2:3]]\n",
        "#     else:\n",
        "#         # Select random content and style images\n",
        "#         content_path = os.path.join(\"data/content\", content_images[0])\n",
        "#         style_path = os.path.join(\"data/style\", style_images[0])\n",
        "#         content_paths = [content_path]\n",
        "#         style_paths = [style_path]\n",
        "\n",
        "#     content_path = content_paths[0]\n",
        "#     style_path = style_paths[0]\n",
        "\n",
        "#     print(f\"\\nContent image: {content_path}\")\n",
        "#     print(f\"Style image: {style_path}\")\n",
        "\n",
        "#     # Load images\n",
        "#     content_img = load_image(content_path)\n",
        "#     style_img = load_image(style_path)\n",
        "\n",
        "#     # Store inputs and results\n",
        "#     results = []\n",
        "\n",
        "#     # Save original images\n",
        "#     content_pil = tensor_to_image(content_img)\n",
        "#     style_pil = tensor_to_image(style_img)\n",
        "#     content_pil.save(\"results/content.jpg\")\n",
        "#     style_pil.save(\"results/style.jpg\")\n",
        "\n",
        "#     results.append((\"Content\", \"results/content.jpg\"))\n",
        "#     results.append((\"Style\", \"results/style.jpg\"))\n",
        "\n",
        "#     # 1. Classic Neural Style Transfer (Gatys et al.)\n",
        "#     # Reduce the number of steps for faster testing\n",
        "#     result_gatys = classic_style_transfer(content_img, style_img, num_steps=150)\n",
        "#     result_gatys.save(\"results/result_gatys.jpg\")\n",
        "#     results.append((\"Gatys et al.\", \"results/result_gatys.jpg\"))\n",
        "\n",
        "#     # 2. Try to use a pretrained Fast Neural Style Transfer model\n",
        "#     pretrained_styles = [ \"mosaic\", \"candy\", \"rain_princess\", \"udnie\", \"wave\", \"tokyo_ghoul\"]\n",
        "#     selected_style = random.choice(pretrained_styles)\n",
        "\n",
        "#     result_pretrained = apply_pretrained_style(content_img, selected_style)\n",
        "#     if result_pretrained is not None:\n",
        "#         result_pretrained.save(f\"results/result_pretrained_{selected_style}.jpg\")\n",
        "#         results.append((f\"Pretrained ({selected_style})\", f\"results/result_pretrained_{selected_style}.jpg\"))\n",
        "\n",
        "#     # 3. AdaIN Style Transfer\n",
        "#     result_adain = adain_style_transfer(content_img, style_img)\n",
        "#     result_adain.save(\"results/result_adain.jpg\")\n",
        "#     results.append((\"AdaIN\", \"results/result_adain.jpg\"))\n",
        "\n",
        "#     # 4. Train a Fast Neural Style Transfer model (Johnson et al.)\n",
        "#     # Note: This is time-consuming, so we'll only do it if explicitly requested\n",
        "#     train_fast = False\n",
        "#     if train_fast:\n",
        "#         transformer = train_fast_style_transfer(style_img, \"data/content\", epochs=2)\n",
        "#         result_fast = apply_fast_style_transfer(content_img, transformer)\n",
        "#         result_fast.save(\"results/result_fast.jpg\")\n",
        "#         results.append((\"Fast NST (Johnson et al.)\", \"results/result_fast.jpg\"))\n",
        "\n",
        "#     # Display results\n",
        "#     print(\"\\nResults saved to 'results' directory\")\n",
        "\n",
        "#     # Create a comparison grid\n",
        "#     fig, axes = plt.subplots(2, len(results)//2 + len(results)%2, figsize=(15, 8))\n",
        "#     axes = axes.flatten()\n",
        "\n",
        "#     for i, (title, img_path) in enumerate(results):\n",
        "#         img = Image.open(img_path)\n",
        "#         axes[i].imshow(np.array(img))\n",
        "#         axes[i].set_title(title)\n",
        "#         axes[i].axis('off')\n",
        "\n",
        "#     # Remove unused subplots\n",
        "#     for i in range(len(results), len(axes)):\n",
        "#         fig.delaxes(axes[i])\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(\"results/comparison.jpg\", dpi=300)\n",
        "#     plt.show()\n",
        "\n",
        "#     print(\"Style transfer comparison complete!\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "8IC-hnsT2i8t",
        "outputId": "3d084a3c-ebb9-4f77-c12d-14f96834866e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading from https://github.com/pytorch/examples/raw/main/fast_neural_style/images.zip...\n",
            "Error downloading/extracting: File is not a zip file\n",
            "Downloading starry_night.jpg...\n",
            "Saved to data/style/starry_night.jpg\n",
            "Downloading the_scream.jpg...\n",
            "Saved to data/style/the_scream.jpg\n",
            "Downloading picasso.jpg...\n",
            "Saved to data/style/picasso.jpg\n",
            "Warning: picasso.jpg may be corrupted\n",
            "Downloading mona_lisa.jpg...\n",
            "Saved to data/style/mona_lisa.jpg\n",
            "Downloading kandinsky.jpg...\n",
            "Saved to data/style/kandinsky.jpg\n",
            "Warning: kandinsky.jpg may be corrupted\n",
            "Downloading great_wave.jpg...\n",
            "Saved to data/style/great_wave.jpg\n",
            "Warning: great_wave.jpg may be corrupted\n",
            "Downloading candy.jpg...\n",
            "Saved to data/style/candy.jpg\n",
            "Warning: candy.jpg may be corrupted\n",
            "\n",
            "Content images:\n",
            "['portrait.jpg', 'beach.jpg', 'landscape.jpg']\n",
            "\n",
            "Style images:\n",
            "['mona_lisa.jpg', 'picasso.jpg', 'great_wave.jpg', 'starry_night.jpg', 'the_scream.jpg', 'candy.jpg', 'kandinsky.jpg']\n",
            "Downloading additional images for content...\n",
            "Downloading landscape.jpg for content...\n",
            "Saved to data/content/landscape.jpg\n",
            "Downloading portrait.jpg for content...\n",
            "Saved to data/content/portrait.jpg\n",
            "Downloading beach.jpg for content...\n",
            "Saved to data/content/beach.jpg\n",
            "\n",
            "Content images:\n",
            "['portrait.jpg', 'beach.jpg', 'landscape.jpg']\n",
            "\n",
            "Content image: data/content/portrait.jpg\n",
            "Style image: data/style/mona_lisa.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_image' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d088dd88e209>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-d088dd88e209>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;31m# Load images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m     \u001b[0mcontent_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0mstyle_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_hub as hub\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_img(path_to_img, max_dim=512):\n",
        "    \"\"\"Load and preprocess images to make them suitable for the model\"\"\"\n",
        "    # Handle different input types\n",
        "    if isinstance(path_to_img, str):\n",
        "        if path_to_img.startswith('http'):\n",
        "            # Download image from URL using requests with a proper User-Agent\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            try:\n",
        "                response = requests.get(path_to_img, headers=headers, timeout=10)\n",
        "                response.raise_for_status()  # Check if the request was successful\n",
        "\n",
        "                # Use PIL to open the image\n",
        "                img = PIL.Image.open(BytesIO(response.content))\n",
        "                # Convert to numpy array and then to tensor\n",
        "                img = np.array(img)\n",
        "                img = tf.convert_to_tensor(img)\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading or processing image from {path_to_img}: {str(e)}\")\n",
        "                raise\n",
        "        else:\n",
        "            try:\n",
        "                img = tf.io.read_file(path_to_img)\n",
        "                img = tf.image.decode_image(img, channels=3)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading local image {path_to_img}: {str(e)}\")\n",
        "                raise\n",
        "    else:\n",
        "        img = path_to_img  # Assume it's already a tensor\n",
        "\n",
        "    # Ensure the image has 3 channels (RGB)\n",
        "    if img.shape[-1] == 4:  # If RGBA (with alpha channel)\n",
        "        img = img[..., :3]  # Keep only RGB\n",
        "\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "    # Resize the image while preserving aspect ratio\n",
        "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "    long_dim = max(shape)\n",
        "    scale = max_dim / long_dim\n",
        "    new_shape = tf.cast(shape * scale, tf.int32)\n",
        "    img = tf.image.resize(img, new_shape)\n",
        "    img = img[tf.newaxis, :]\n",
        "    return img\n",
        "\n",
        "# Function to show images\n",
        "def imshow(image, title=None):\n",
        "    \"\"\"Display an image with an optional title\"\"\"\n",
        "    if len(image.shape) > 3:\n",
        "        image = tf.squeeze(image, axis=0)\n",
        "    plt.imshow(image)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "\n",
        "# Function to save images\n",
        "def save_img(image, filename):\n",
        "    \"\"\"Save the generated image to disk\"\"\"\n",
        "    if len(image.shape) > 3:\n",
        "        image = tf.squeeze(image, axis=0)\n",
        "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
        "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
        "    image = PIL.Image.fromarray(np.array(image))\n",
        "    image.save(filename)\n",
        "    print(f\"Image saved to {filename}\")\n",
        "\n",
        "# Function to use TensorFlow Hub model\n",
        "def style_transfer_hub(content_path, style_path, output_path):\n",
        "    \"\"\"Use a pre-trained TensorFlow Hub model for style transfer\"\"\"\n",
        "    try:\n",
        "        # Load the model\n",
        "        hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "\n",
        "        # Load images\n",
        "        content_image = load_img(content_path, max_dim=512)\n",
        "        style_image = load_img(style_path, max_dim=256)\n",
        "\n",
        "        # Generate styled image\n",
        "        outputs = hub_model(tf.constant(content_image), tf.constant(style_image))\n",
        "        stylized_image = outputs[0]\n",
        "\n",
        "        # Save the image\n",
        "        save_img(stylized_image, output_path)\n",
        "        return stylized_image\n",
        "    except Exception as e:\n",
        "        print(f\"Error during style transfer: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Create output directory\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "\n",
        "    # Define content and style image URLs - using more reliable image URLs\n",
        "    content_images = [\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/800px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\"\n",
        "    ]\n",
        "\n",
        "    style_images = [\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/The_Great_Wave_off_Kanagawa.jpg/1024px-The_Great_Wave_off_Kanagawa.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/800px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\"\n",
        "    ]\n",
        "\n",
        "    print(f\"Using {len(content_images)} content images and {len(style_images)} style images from online sources\")\n",
        "\n",
        "    # Run style transfer using TensorFlow Hub model for all combinations\n",
        "    print(\"\\nRunning TensorFlow Hub style transfer for selected combinations...\")\n",
        "\n",
        "    # Process a few combinations\n",
        "    combinations = [\n",
        "        (0, 0),  # Van Gogh's Starry Night with The Great Wave style\n",
        "        (1, 0)   # Mona Lisa with The Great Wave style\n",
        "    ]\n",
        "\n",
        "    for i, j in combinations:\n",
        "        output_path = f\"output/stylized_content{i}_style{j}.jpg\"\n",
        "        print(f\"\\nApplying style {j} to content {i}...\")\n",
        "        try:\n",
        "            stylized_hub = style_transfer_hub(content_images[i], style_images[j], output_path)\n",
        "            print(f\"Successfully created stylized image at {output_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process combination {i},{j}: {str(e)}\")\n",
        "\n",
        "    print(\"\\nStyle transfer complete! Results saved in the 'output' directory.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCfQatPOi7MV",
        "outputId": "094dcd43-924e-4d7d-c7d3-c9540a48ebfd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 2 content images and 2 style images from online sources\n",
            "\n",
            "Running TensorFlow Hub style transfer for selected combinations...\n",
            "\n",
            "Applying style 0 to content 0...\n",
            "Image saved to output/stylized_content0_style0.jpg\n",
            "Successfully created stylized image at output/stylized_content0_style0.jpg\n",
            "\n",
            "Applying style 0 to content 1...\n",
            "Image saved to output/stylized_content1_style0.jpg\n",
            "Successfully created stylized image at output/stylized_content1_style0.jpg\n",
            "\n",
            "Style transfer complete! Results saved in the 'output' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSYKfMEZJ7p0"
      },
      "source": [
        "## **Task 5-3**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Hyperparameters\n",
        "# ---------------------------\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.001\n",
        "EPOCHS = 5\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Data Preparation\n",
        "# ---------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "# Download CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Define a Simple CNN Model\n",
        "# ---------------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "        # Activation & dropout\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1st conv block\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)  # 32 -> 16 (image size)\n",
        "\n",
        "        # 2nd conv block\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)  # 16 -> 8 (image size)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)  # 64 * 8 * 8 = 4096\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, define loss and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Training\n",
        "# ---------------------------\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Evaluation on Test Set\n",
        "# ---------------------------\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "test_accuracy = 100.0 * test_correct / test_total\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Plot Training Loss & Accuracy\n",
        "#    (Each in its own chart)\n",
        "# ---------------------------\n",
        "\n",
        "# Plot training loss\n",
        "# plt.figure()\n",
        "# plt.plot(range(1, EPOCHS+1), train_losses, marker='o')\n",
        "# plt.title(\"Training Loss\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.show()\n",
        "\n",
        "# # Plot training accuracy\n",
        "# plt.figure()\n",
        "# plt.plot(range(1, EPOCHS+1), train_accuracies, marker='o')\n",
        "# plt.title(\"Training Accuracy\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Accuracy (%)\")\n",
        "# plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, EPOCHS+1), train_losses, marker='o')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig(\"training_loss.png\")  # saves the figure\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, EPOCHS+1), train_accuracies, marker='o')\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.savefig(\"training_accuracy.png\")  # saves the figure\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mwEYcAzFta3",
        "outputId": "501b73f2-a993-411a-ce41-7b8be96e69fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/5], Loss: 1.5587, Accuracy: 42.92%\n",
            "Epoch [2/5], Loss: 1.2411, Accuracy: 55.40%\n",
            "Epoch [3/5], Loss: 1.1147, Accuracy: 60.37%\n",
            "Epoch [4/5], Loss: 1.0376, Accuracy: 63.22%\n",
            "Epoch [5/5], Loss: 0.9905, Accuracy: 65.20%\n",
            "Test Accuracy: 67.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "# Set up model paths and configurations\n",
        "CONFIDENCE_THRESHOLD = 0.5\n",
        "\n",
        "class ParkingSpaceDetector:\n",
        "    def __init__(self):\n",
        "        # Load YOLOv5 model\n",
        "        print(\"Loading YOLOv5 model...\")\n",
        "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, trust_repo=True)\n",
        "\n",
        "        # Configure model settings\n",
        "        self.model.conf = CONFIDENCE_THRESHOLD\n",
        "        self.model.classes = [2, 5, 7]  # Filter for cars, buses, trucks\n",
        "\n",
        "        # Load parking space configurations\n",
        "        self.parking_spaces = self.load_parking_spaces()\n",
        "\n",
        "        # Initialize results storage\n",
        "        self.results_history = []\n",
        "\n",
        "    def load_parking_spaces(self):\n",
        "        \"\"\"\n",
        "        Load parking space coordinates from config file.\n",
        "        For demonstration, we'll use fixed coordinates.\n",
        "        In production, these would be loaded from a JSON file.\n",
        "        \"\"\"\n",
        "        # Example parking space coordinates (x1, y1, x2, y2)\n",
        "        # In production, these would be defined during setup phase\n",
        "        return [\n",
        "            {\"id\": 1, \"coords\": (100, 200, 150, 250)},\n",
        "            {\"id\": 2, \"coords\": (160, 200, 210, 250)},\n",
        "            {\"id\": 3, \"coords\": (220, 200, 270, 250)},\n",
        "            {\"id\": 4, \"coords\": (280, 200, 330, 250)},\n",
        "            {\"id\": 5, \"coords\": (100, 260, 150, 310)},\n",
        "            {\"id\": 6, \"coords\": (160, 260, 210, 310)},\n",
        "            {\"id\": 7, \"coords\": (220, 260, 270, 310)},\n",
        "            {\"id\": 8, \"coords\": (280, 260, 330, 310)},\n",
        "        ]\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        \"\"\"Process a single video frame to detect parking space occupancy\"\"\"\n",
        "        # Run YOLOv5 on the frame\n",
        "        results = self.model(frame)\n",
        "\n",
        "        # Get the detected vehicles\n",
        "        vehicles = results.pandas().xyxy[0]\n",
        "\n",
        "        # Check each parking space\n",
        "        occupancy_status = {}\n",
        "        for space in self.parking_spaces:\n",
        "            space_id = space[\"id\"]\n",
        "            x1, y1, x2, y2 = space[\"coords\"]\n",
        "\n",
        "            # Check if any vehicle overlaps with this parking space\n",
        "            is_occupied = False\n",
        "\n",
        "            for _, vehicle in vehicles.iterrows():\n",
        "                v_x1, v_y1, v_x2, v_y2 = vehicle[['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "\n",
        "                # Calculate overlap (IoU - Intersection over Union)\n",
        "                overlap = self.calculate_iou(\n",
        "                    (x1, y1, x2, y2),\n",
        "                    (v_x1, v_y1, v_x2, v_y2)\n",
        "                )\n",
        "\n",
        "                if overlap > 0.2:  # If overlap is significant\n",
        "                    is_occupied = True\n",
        "                    break\n",
        "\n",
        "            occupancy_status[space_id] = is_occupied\n",
        "\n",
        "        # Store result with timestamp\n",
        "        self.results_history.append({\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"occupancy\": occupancy_status\n",
        "        })\n",
        "\n",
        "        return occupancy_status, frame\n",
        "\n",
        "    def calculate_iou(self, box1, box2):\n",
        "        \"\"\"Calculate Intersection over Union between two bounding boxes\"\"\"\n",
        "        # Determine intersection coordinates\n",
        "        x1 = max(box1[0], box2[0])\n",
        "        y1 = max(box1[1], box2[1])\n",
        "        x2 = min(box1[2], box2[2])\n",
        "        y2 = min(box1[3], box2[3])\n",
        "\n",
        "        # Calculate area of intersection\n",
        "        width = max(0, x2 - x1)\n",
        "        height = max(0, y2 - y1)\n",
        "        intersection_area = width * height\n",
        "\n",
        "        # Calculate areas of both boxes\n",
        "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "        # Calculate IoU\n",
        "        union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "        if union_area == 0:\n",
        "            return 0\n",
        "\n",
        "        return intersection_area / union_area\n",
        "\n",
        "    def visualize_results(self, frame, occupancy_status):\n",
        "        \"\"\"Draw parking spaces and their status on the frame\"\"\"\n",
        "        result_frame = frame.copy()\n",
        "\n",
        "        for space in self.parking_spaces:\n",
        "            space_id = space[\"id\"]\n",
        "            x1, y1, x2, y2 = space[\"coords\"]\n",
        "\n",
        "            # Choose color based on occupancy (green for empty, red for occupied)\n",
        "            color = (0, 255, 0) if not occupancy_status[space_id] else (0, 0, 255)\n",
        "\n",
        "            # Draw rectangle for parking space\n",
        "            cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "            # Add space ID label\n",
        "            cv2.putText(result_frame, str(space_id), (x1 + 5, y1 + 20),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "        # Add summary information\n",
        "        available_count = sum(1 for status in occupancy_status.values() if not status)\n",
        "        total_count = len(occupancy_status)\n",
        "\n",
        "        cv2.putText(result_frame, f\"Available: {available_count}/{total_count}\",\n",
        "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "        return result_frame\n",
        "\n",
        "    def get_occupancy_stats(self):\n",
        "        \"\"\"Generate statistics about parking space usage over time\"\"\"\n",
        "        if not self.results_history:\n",
        "            return \"No data available yet\"\n",
        "\n",
        "        # Calculate how often each space is occupied\n",
        "        space_ids = [space[\"id\"] for space in self.parking_spaces]\n",
        "        occupancy_rates = {space_id: 0 for space_id in space_ids}\n",
        "\n",
        "        for record in self.results_history:\n",
        "            occupancy = record[\"occupancy\"]\n",
        "            for space_id, is_occupied in occupancy.items():\n",
        "                if is_occupied:\n",
        "                    occupancy_rates[space_id] += 1\n",
        "\n",
        "        # Convert to percentages\n",
        "        total_records = len(self.results_history)\n",
        "        for space_id in occupancy_rates:\n",
        "            occupancy_rates[space_id] = (occupancy_rates[space_id] / total_records) * 100\n",
        "\n",
        "        return occupancy_rates\n",
        "\n",
        "def download_sample_images():\n",
        "    \"\"\"\n",
        "    Download sample parking lot images from the internet for demonstration\n",
        "    Returns a list of images as numpy arrays\n",
        "    \"\"\"\n",
        "    print(\"Downloading sample parking lot images...\")\n",
        "\n",
        "    # URLs for sample parking lot images\n",
        "    image_urls = [\n",
        "        \"https://raw.githubusercontent.com/mbeyeler/opencv-python-blueprints/master/chapter4/images/parking-lot-img-224.jpg\",\n",
        "        \"https://raw.githubusercontent.com/mbeyeler/opencv-python-blueprints/master/chapter4/images/parking-lot-img-303.jpg\",\n",
        "        \"https://raw.githubusercontent.com/mbeyeler/opencv-python-blueprints/master/chapter4/images/parking-lot-img-407.jpg\",\n",
        "        \"https://cdn.shopify.com/s/files/1/0196/4614/articles/How_can_Smart_Parking_Solutions_help_Solve_Parking_Problems_1400x.png\",\n",
        "        \"https://images.unsplash.com/photo-1525126997622-d5c1bb293dfe\"\n",
        "    ]\n",
        "\n",
        "    images = []\n",
        "\n",
        "    for url in image_urls:\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                # Convert to image and then to numpy array\n",
        "                img = Image.open(BytesIO(response.content))\n",
        "                img_array = np.array(img)\n",
        "\n",
        "                # Convert to RGB if needed\n",
        "                if len(img_array.shape) == 2:  # Grayscale image\n",
        "                    img_array = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
        "                elif img_array.shape[2] == 4:  # RGBA image\n",
        "                    img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "                images.append(img_array)\n",
        "                print(f\"Successfully downloaded image from {url}\")\n",
        "            else:\n",
        "                print(f\"Failed to download image from {url}: Status code {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading image from {url}: {e}\")\n",
        "\n",
        "    if not images:\n",
        "        # Generate synthetic images if no real images could be downloaded\n",
        "        print(\"Generating synthetic parking lot images...\")\n",
        "        for _ in range(3):\n",
        "            # Create a simple synthetic image\n",
        "            img = np.ones((500, 800, 3), dtype=np.uint8) * 120  # Gray background\n",
        "\n",
        "            # Draw some parking spaces\n",
        "            for i in range(4):\n",
        "                for j in range(6):\n",
        "                    x = 100 + j * 100\n",
        "                    y = 100 + i * 80\n",
        "                    # Randomly decide if parking space is occupied\n",
        "                    if np.random.rand() > 0.5:\n",
        "                        # Draw a car (rectangle)\n",
        "                        cv2.rectangle(img, (x, y), (x + 80, y + 60), (0, 0, 200), -1)\n",
        "                    else:\n",
        "                        # Draw empty parking space\n",
        "                        cv2.rectangle(img, (x, y), (x + 80, y + 60), (100, 100, 100), 2)\n",
        "\n",
        "            images.append(img)\n",
        "\n",
        "    print(f\"Total images ready for processing: {len(images)}\")\n",
        "    return images\n",
        "\n",
        "def demo_with_sample_images(detector):\n",
        "    \"\"\"Run detection on a set of sample images from the web or synthetic images\"\"\"\n",
        "    # Get sample images\n",
        "    sample_images = download_sample_images()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for i, img in enumerate(sample_images[:3]):  # Process at most 3 images for demo\n",
        "        # Process frame\n",
        "        occupancy, _ = detector.process_frame(img)\n",
        "\n",
        "        # Visualize results\n",
        "        result_frame = detector.visualize_results(img, occupancy)\n",
        "\n",
        "        # Display\n",
        "        plt.subplot(len(sample_images[:3]), 1, i+1)\n",
        "        plt.imshow(result_frame)\n",
        "        plt.title(f\"Sample {i+1}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('parking_detection_results.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Print occupancy stats\n",
        "    print(\"\\nParking Space Occupancy Rates:\")\n",
        "    occupancy_rates = detector.get_occupancy_stats()\n",
        "\n",
        "    if isinstance(occupancy_rates, dict):\n",
        "        for space_id, rate in occupancy_rates.items():\n",
        "            print(f\"Space {space_id}: {rate:.1f}% occupied\")\n",
        "    else:\n",
        "        print(occupancy_rates)  # In case it returns a message like \"No data available yet\"\n",
        "\n",
        "# Usage example for demo\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Initializing Parking Space Detector...\")\n",
        "    # Initialize detector\n",
        "    detector = ParkingSpaceDetector()\n",
        "\n",
        "    # Run the demo\n",
        "    print(\"Running detection on sample images...\")\n",
        "    demo_with_sample_images(detector)\n",
        "\n",
        "    print(\"Demo complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBky0qShHOik",
        "outputId": "13163f43-02bd-45cf-c9d1-da92da68966c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2025-3-6 Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Parking Space Detector...\n",
            "Loading YOLOv5 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running detection on sample images...\n",
            "Downloading sample parking lot images...\n",
            "Failed to download image from https://raw.githubusercontent.com/mbeyeler/opencv-python-blueprints/master/chapter4/images/parking-lot-img-224.jpg: Status code 404\n",
            "Failed to download image from https://raw.githubusercontent.com/mbeyeler/opencv-python-blueprints/master/chapter4/images/parking-lot-img-303.jpg: Status code 404\n",
            "Failed to download image from https://raw.githubusercontent.com/mbeyeler/opencv-python-blueprints/master/chapter4/images/parking-lot-img-407.jpg: Status code 404\n",
            "Failed to download image from https://cdn.shopify.com/s/files/1/0196/4614/articles/How_can_Smart_Parking_Solutions_help_Solve_Parking_Problems_1400x.png: Status code 404\n",
            "Failed to download image from https://images.unsplash.com/photo-1525126997622-d5c1bb293dfe: Status code 404\n",
            "Generating synthetic parking lot images...\n",
            "Total images ready for processing: 3\n",
            "\n",
            "Parking Space Occupancy Rates:\n",
            "Space 1: 0.0% occupied\n",
            "Space 2: 0.0% occupied\n",
            "Space 3: 0.0% occupied\n",
            "Space 4: 0.0% occupied\n",
            "Space 5: 0.0% occupied\n",
            "Space 6: 0.0% occupied\n",
            "Space 7: 0.0% occupied\n",
            "Space 8: 0.0% occupied\n",
            "Demo complete!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN2VPwRiqJvDz/KsLN0nP9n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}